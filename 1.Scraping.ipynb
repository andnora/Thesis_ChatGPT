{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Reddit for mentions of ChatGPT in Posts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements for the scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting authorized by filling in personal reddit account details\n",
    "reddit_authorized = praw.Reddit(client_id=\"#############\",\n",
    "                                client_secret=\"############\",\n",
    "                                user_agent=\"Getting_Scraped_By_############\",\n",
    "                                username=\"#############\",\n",
    "                                password=\"#############\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the posts from the subreddit(s)\n",
    "def get_subreddit_data(subreddits:praw.models.Subreddits):\n",
    "    \"\"\"\n",
    "    Assign a PRAW subreddit object for subreddits:praw.models.Subreddits\n",
    "    Manipulates the information about the subreddits to obtain what we want\n",
    "    Output: A Pandas DataFrame containing information on the subreddit's posts\n",
    "    \"\"\"\n",
    "    #Placing the data in a list\n",
    "    data_subreddits:list = []\n",
    "    for submission in subreddits:\n",
    "        data_subreddits.append([submission.id,\n",
    "                     submission.subreddit.display_name,\n",
    "                     submission.title,\n",
    "                     submission.selftext,\n",
    "                     submission.author,\n",
    "                     submission.score,\n",
    "                     submission.num_comments,\n",
    "                     submission.shortlink,\n",
    "                     dt.datetime.fromtimestamp(submission.created_utc)\n",
    "                     ])\n",
    "\n",
    "    #Creating a dataframe with the obtained information\n",
    "    df_posts:pd.DataFrame = pd.DataFrame(data_subreddits, columns=['ID of Post',\n",
    "                                                     'Subreddit',\n",
    "                                                     'Title Post',\n",
    "                                                     'Post Text',\n",
    "                                                     'Author',\n",
    "                                                     'Score', #upvotes - downvotes\n",
    "                                                     'Number of Comments',\n",
    "                                                     'URL of Post',\n",
    "                                                     'Date & Time'\n",
    "                                                     ])\n",
    "    return df_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all the comments per post\n",
    "def get_subreddit_comments(reddit_authorized, df_posts):\n",
    "    \"\"\"\n",
    "    Takes in a PRAW authorized instance and a Pandas DataFrame of post IDs,\n",
    "    returns a Pandas DataFrame containing information on the subreddit's comments.\n",
    "    \"\"\"\n",
    "    data_comments:list = []\n",
    "    post_id:str = ''\n",
    "    for post_id in tqdm(df_posts[\"ID of Post\"], total=len(df_posts), desc=\"Getting Comments\"):\n",
    "        post = reddit_authorized.submission(id=post_id)\n",
    "        post.comments.replace_more(limit=None, threshold=0)\n",
    "        for comment in post.comments.list():\n",
    "            if type(comment) == MoreComments:\n",
    "                continue\n",
    "            if \"AutoModerator\" in str(comment.author):\n",
    "                continue\n",
    "            data_comments.append([post.subreddit.display_name,\n",
    "                                  post.id,\n",
    "                                  comment.body,\n",
    "                                  comment.score,\n",
    "                                  comment.author,\n",
    "                                  dt.datetime.fromtimestamp(comment.created)])\n",
    "    \n",
    "    # Creating the dataframe\n",
    "    df_comments:pd.DataFrame = pd.DataFrame(data_comments, columns=[\n",
    "                                'Subreddit',\n",
    "                                'ID of Post',\n",
    "                                'Comment Text',\n",
    "                                'Score',\n",
    "                                'Author',\n",
    "                                'Date & Time'])\n",
    "    return df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting text from the images and articles from the URL's in the posts\n",
    "def get_text_from_URL(df_submissions:pd.DataFrame, reddit_authorized):\n",
    "    \"\"\"\n",
    "    Assign a PRAW authorized instance for reddit_authorized\n",
    "    Assign a Pandas DataFrame for df_submissions\n",
    "    Creates a new column in the original dataframe with the text from the image or article\n",
    "    Output: A Pandas DataFrame containing more information on the subreddit's posts\n",
    "    \"\"\"\n",
    "    new_columns:list = []\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    #Loop through each row in the dataset\n",
    "    for index, row in tqdm(df_submissions.iterrows(), total=len(df_submissions), desc=\"Processing URL of Post\"):\n",
    "        post_url = row['URL of Post']\n",
    "        url_submission = reddit_authorized.submission(url=post_url)\n",
    "        #Check if the link in the post goes to an image\n",
    "        if url_submission.url.endswith(('jpg', 'jpeg', 'png', 'gif')):\n",
    "            response = requests.get(url_submission.url)\n",
    "            img = Image.open(io.BytesIO(response.content))\n",
    "            # image_text = pytesseract.image_to_string(img)\n",
    "            # new_columns.append(image_text)\n",
    "            img_np = np.array(img)\n",
    "            results = reader.readtext(img_np)\n",
    "            image_text = ' '.join([result[1] for result in results])\n",
    "            new_columns.append(image_text)\n",
    "        #Check if the link in the post goes to a news article\n",
    "        elif url_submission.url.startswith(('http://', 'https://')):\n",
    "            response = requests.get(url_submission.url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_text = ''\n",
    "            for paragraph in soup.find_all('p'):\n",
    "                article_text += paragraph.text\n",
    "            new_columns.append(article_text)\n",
    "        #If the link does not go to an image or article, add an empty string\n",
    "        else:\n",
    "            new_columns.append('')\n",
    "    #Add the new column to the original dataframe\n",
    "    df_submissions['Text of URL Post'] = new_columns\n",
    "    #Return a new df with the same columns as the original, plus the new column\n",
    "    return df_submissions.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting subreddits to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching through all the subreddits\n",
    "subs_mention_chatgpt = reddit_authorized.subreddit('all').search('ChatGPT', time_filter='year', limit=None)\n",
    "#Removing the posts from the ChatGPT subreddit\n",
    "subs_mention_chatgpt = [submission for submission in subs_mention_chatgpt if submission.subreddit.display_name.lower() != 'chatgpt']\n",
    "#Only saving the posts from specific time period\n",
    "subs_mention_chatgpt = [submission for submission in subs_mention_chatgpt if\n",
    "                              dt.datetime.fromtimestamp(submission.created_utc) > dt.datetime(year=2022, month=11, day=29) and\n",
    "                              dt.datetime.fromtimestamp(submission.created_utc) < dt.datetime(year=2023, month=4, day=21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the subreddit ChatGPT\n",
    "sub_chatgpt = reddit_authorized.subreddit('ChatGPT')\n",
    "sub_chatgpt = sub_chatgpt.top(time_filter='year', limit=None)\n",
    "#Only saving the posts from specific time period\n",
    "sub_chatgpt = [submission for submission in sub_chatgpt if\n",
    "                              dt.datetime.fromtimestamp(submission.created_utc) > dt.datetime(year=2022, month=11, day=29) and\n",
    "                              dt.datetime.fromtimestamp(submission.created_utc) < dt.datetime(year=2023, month=4, day=21)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for splitting the mentions of ChatGPT in subreddits and the ChatGPT subreddit is because of the large amount of posts in the ChatGPT subreddit. By using this method, a maximum of a 1000 posts can be scraped. This means the top 1000 posts (based on score) of the given time period. Because we are splitting the datasets, we can get more information on the conversation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the scraping and saving the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URL of Post: 100%|██████████| 93/93 [08:47<00:00,  5.67s/it]\n",
      "CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Processing URL of Post:  41%|████      | 362/892 [1:15:58<1:51:14, 12.59s/it]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'img' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m df_sub_chatgpt \u001b[39m=\u001b[39m get_subreddit_data(sub_chatgpt)\n\u001b[0;32m      5\u001b[0m df_subs_ment_chatgpt \u001b[39m=\u001b[39m get_text_from_URL(df_subs_ment_chatgpt, reddit_authorized)\n\u001b[1;32m----> 6\u001b[0m df_sub_chatgpt \u001b[39m=\u001b[39m get_text_from_URL(df_sub_chatgpt, reddit_authorized)\n\u001b[0;32m      8\u001b[0m df_subs_comments \u001b[39m=\u001b[39m get_subreddit_comments(reddit_authorized, df_subs_ment_chatgpt)\n\u001b[0;32m      9\u001b[0m df_chatgpt_comments \u001b[39m=\u001b[39m get_subreddit_comments(reddit_authorized, df_sub_chatgpt)\n",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m, in \u001b[0;36mget_text_from_URL\u001b[1;34m(df_submissions, reddit_authorized)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39m# image_text = pytesseract.image_to_string(img)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# new_columns.append(image_text)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m img_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(img)\n\u001b[1;32m---> 22\u001b[0m results \u001b[39m=\u001b[39m reader\u001b[39m.\u001b[39;49mreadtext(img_np)\n\u001b[0;32m     23\u001b[0m image_text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([result[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m results])\n\u001b[0;32m     24\u001b[0m new_columns\u001b[39m.\u001b[39mappend(image_text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\easyocr\\easyocr.py:442\u001b[0m, in \u001b[0;36mReader.readtext\u001b[1;34m(self, image, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, min_size, contrast_ths, adjust_contrast, filter_ths, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, y_ths, x_ths, add_margin, threshold, bbox_min_score, bbox_min_size, max_candidates, output_format)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadtext\u001b[39m(\u001b[39mself\u001b[39m, image, decoder \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgreedy\u001b[39m\u001b[39m'\u001b[39m, beamWidth\u001b[39m=\u001b[39m \u001b[39m5\u001b[39m, batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\\\n\u001b[0;32m    429\u001b[0m              workers \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, allowlist \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, blocklist \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, detail \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\\\n\u001b[0;32m    430\u001b[0m              rotation_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, paragraph \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, min_size \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m,\\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    436\u001b[0m              threshold \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m, bbox_min_score \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m, bbox_min_size \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, max_candidates \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[0;32m    437\u001b[0m              output_format\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstandard\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    438\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[39m    Parameters:\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[39m    image: file path or numpy-array or a byte stream object\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     img, img_cv_grey \u001b[39m=\u001b[39m reformat_input(image)\n\u001b[0;32m    444\u001b[0m     horizontal_list, free_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetect(img, \n\u001b[0;32m    445\u001b[0m                                              min_size \u001b[39m=\u001b[39m min_size, text_threshold \u001b[39m=\u001b[39m text_threshold,\\\n\u001b[0;32m    446\u001b[0m                                              low_text \u001b[39m=\u001b[39m low_text, link_threshold \u001b[39m=\u001b[39m link_threshold,\\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m                                              bbox_min_size \u001b[39m=\u001b[39m bbox_min_size, max_candidates \u001b[39m=\u001b[39m max_candidates\n\u001b[0;32m    453\u001b[0m                                              )\n\u001b[0;32m    454\u001b[0m     \u001b[39m# get the 1st result from hor & free list as self.detect returns a list of depth 3\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\easyocr\\utils.py:727\u001b[0m, in \u001b[0;36mreformat_input\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    725\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInvalid input type. Supporting format = string(file path or url), bytes, numpy array\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 727\u001b[0m \u001b[39mreturn\u001b[39;00m img, img_cv_grey\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'img' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#Calling the functions\n",
    "df_subs_ment_chatgpt = get_subreddit_data(subs_mention_chatgpt)\n",
    "df_sub_chatgpt = get_subreddit_data(sub_chatgpt)\n",
    "\n",
    "df_subs_ment_chatgpt = get_text_from_URL(df_subs_ment_chatgpt, reddit_authorized)\n",
    "df_sub_chatgpt = get_text_from_URL(df_sub_chatgpt, reddit_authorized)\n",
    "\n",
    "df_subs_comments = get_subreddit_comments(reddit_authorized, df_subs_ment_chatgpt)\n",
    "df_chatgpt_comments = get_subreddit_comments(reddit_authorized, df_sub_chatgpt)\n",
    "\n",
    "#Saving the dataframes to csv files\n",
    "df_subs_ment_chatgpt.to_csv('F1sub_subreddits.csv', index=False)\n",
    "df_sub_chatgpt.to_csv('F1sub_chatgpt.csv', index=False)\n",
    "df_subs_comments.to_csv('F1comments_subreddits.csv', index=False)\n",
    "df_chatgpt_comments.to_csv('F1comments_chatgpt.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
